{% load_yaml as lookup_map %}
default:
  service: spark-worker
  user: spark
  prefix: /usr/local
  config_dir: /etc/spark
  log_dir: /var/log/spark
  version: "2.1.0"
  hadoop_version: "2.7"
  with_hadoop: true
  archive_url: "http://www.apache.org/dist/spark/spark-%(version)s"
  archive_type: "tgz"
Debian: {}
RedHat: {}
{% endload %}

{% load_yaml as versions %}
with_hadoop:
  "2.1.0":
    name: "spark-%(version)s-bin-hadoop%(hadoop_version)s"
    hash: "sha256=0834c775f38473f67cb122e0ec21074f800ced50c1ff1b9e37e222a0069dc5c7"
without_hadoop:
  "2.1.0":
    name: "spark-%(version)s-bin-without-hadoop"
    hash: "sha256=3ca4ecb0eb9a00de5099cc2564ed957433a2d15d9d645a60470324621853c5ae"
{% endload %}

# http://spark.apache.org/docs/latest/configuration.html#spark-properties
{% load_yaml as config %}
master: ~
extraListeners: ~
logConf: false
  
local:
  dir: /tmp

app:
  name: super-cool-app

driver:
  cores: 1
  maxResultSize: 1g
  memory: 1g
  
executor:
  memory: 1g
  logs:
    rolling:
      maxRetainedFiles: ~
      enableCompression: false
      maxSize: ~
      strategy: ~
      time.interval: daily
      userClassPathFirst: false

python:
  worker:
    memory: 512m
    reuse: true
    
reducer:
  maxSizeInFlight: 48m

shuffle:
  compress: true
  file:
    buffer: 32k

eventLog:
  compress: false
  dir: file:///tmp/spark-events
  enabled: false

ui:
  enabled: true
  killEnabled: true
  port: 4040
  retainedJobs: 1000
  retainedStages: 1000
  retainedTasks: 100000
  reverseProxy: false

worker:
  ui:
    retainedExecutors: 1000
    retainedDrivers: 1000
    retainedExecutions: 1000
    retainedDeadExecutors: 100


    
{% endload %}
