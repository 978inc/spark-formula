{% load_yaml as lookup_map %}
default:
  worker_service: 'spark-worker'
  master_service: 'spark-master'
  user: spark
  prefix: /usr/local/lib
  config_dir: /etc/spark
  log_dir: /var/log/spark
  work_dir: /tmp/spark
  pid_dir: /var/run/spark
  niceness: 0
  
  # controls the version of spark to be installed
  version: "2.2.0"
  # used to build the artifact filename
  hadoop_version: "2.7"
  # set to false if the "without-hadoop" version is desired
  with_hadoop: true
  # 
  archive_type: "tgz"

  # use a mirror to fetch the scala artifact
  archive_url_base: http://download.nextag.com/apache/spark
  # use the official servers to pull in Apache GPG keys and artifact signatures
  archive_hash_url_base: https://archive.apache.org/dist/spark

  # used for minion targeting and master discovery
  worker_role: 'spark-worker'
  master_role: 'spark-master'
  
  master_host: 127.0.0.1
  master_port: 7077
  master_ui_port: 8080
  master_opts: []
  local_dirs: /tmp/spark/scratch
  
  worker_cores: 0 # all cores
  worker_memory: 2g
  worker_instances: 1
  worker_webui_port: 8081
  worker_dir: ~
  worker_opts: []
  daemon_memory: 1g
  daemon_java_opts: []
  public_dns: ~

  init_system: ~
  init_scripts: ~
  init_overrides: ~

  package_deps: []
    
Debian:
  init_system: systemd
  init_scripts: /etc/systemd/system
  init_overrides: /etc/default
  package_deps:
    - scala
    - git
  java_deps:
    - default-jre
    - default-jdk
RedHat:
  init_system: systemd
  init_scripts: /usr/lib/systemd/system
  init_overrides: /etc/systemd/system
  package_deps:
    - scala
    - git
  java_deps:
    - java-1.8.0-openjdk
    - java-1.8.0-openjdk-devel
{% endload %}
